{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution: API Limitations for LLM deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LinkedInLearning/generative-ai-and-llmops-deploying-and-managing-llms-in-production-4465782/blob/solution/ch-02/challenge_API_limitations.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d0-Q5FXjfKHK",
    "outputId": "01167a30-98aa-4d21-bb6f-b7562b284545"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/320.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/320.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install groq openai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9esvOP1ap8R",
    "outputId": "f2940942-e3d1-4675-f0f0-f83d981ebde6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "··········\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "import getpass\n",
    "\n",
    "# Get token from console.groq.com\n",
    "client = Groq(api_key=getpass.getpass())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMXH-N19gW23"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, prompt):\n",
    "    # TODO\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        model=model\n",
    "    )\n",
    "    return chat_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nUGTqYWtgdI_",
    "outputId": "d5664fef-a3a7-4e67-bcd3-3ca0df875219"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: llama3-8b-8192\n",
      "Total tokens: 680\n",
      "Total time: 0.75\n",
      "Throughput: 906.6666666666666\n",
      "-----------\n",
      "Model: llama3-70b-8192\n",
      "Total tokens: 781\n",
      "Total time: 2.439\n",
      "Throughput: 320.2132021320213\n",
      "-----------\n",
      "Model: mixtral-8x7b-32768\n",
      "Total tokens: 1016\n",
      "Total time: 1.729919124\n",
      "Throughput: 587.3106932599006\n",
      "-----------\n",
      "Model: gemma-7b-it\n",
      "Total tokens: 483\n",
      "Total time: 0.576\n",
      "Throughput: 838.5416666666667\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "model_list=['llama3-8b-8192', 'llama3-70b-8192', 'mixtral-8x7b-32768', 'gemma-7b-it']\n",
    "prompt=\"Write a blog about taking Generative AI applications to production\"\n",
    "\n",
    "for model in model_list:\n",
    "    chat_completion = generate_text(model, prompt)\n",
    "    total_tokens=chat_completion.usage.total_tokens\n",
    "    total_time=chat_completion.usage.completion_time\n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"Total tokens: {total_tokens}\")\n",
    "    print(f\"Total time: {total_time}\")\n",
    "    print(f\"Throughput: {total_tokens/total_time}\")\n",
    "    print(\"-----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hLOlOqGka_U"
   },
   "source": [
    "## OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2DaHkXtsibge",
    "outputId": "9d1fce71-5a37-416a-c87a-030acac2cd7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "··········\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "client = OpenAI(api_key=getpass.getpass())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZCYV1Dv2keKx",
    "outputId": "f3bdfb1e-d39c-43de-9a18-95b05fc1e03f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"### Unleashing the Potential: Taking Generative AI Applications to Production\\n\\nThe rise of generative AI has been nothing short of transformative, influencing sectors from creative industries to technical fields. Technologies like GPT (Generative Pre-trained Transformer), DALLE, and other AI models have shown impressive capabilities in generating human-like text, crafting images, or composing music. As these AI-driven innovations transition from experimental to operational, there are several key considerations and steps organizations must undertake to successfully integrate and scale generative AI applications in production environments.\\n\\n#### Understanding Generative AI\\n\\nGenerative AI refers to a class of artificial intelligence that can generate new content based on the patterns and information it has learned during training. These models are typically built using neural networks that have been trained on vast datasets, enabling them to produce outputs that are convincingly realistic and contextually appropriate.\\n\\n#### 1. Clearly Define Use Cases and Expectations\\n\\nBefore moving a generative AI application to production, it's crucial to identify and clearly define its use cases. Businesses should determine how these applications align with their strategic goals and what problems they are expected to solve. Whether it’s automating content creation, providing personalized customer experiences, or optimizing operational processes, having a clear deployment strategy is vital.\\n\\n#### 2. Focus on Data Quality and Diversity\\n\\nThe performance of generative AI models heavily depends on the quality and diversity of the training data. When preparing for production, ensure that the data used is not only high in quality but also representative of diverse scenarios and conditions under which the AI will operate. This preparation helps in minimizing biases and enhances the ability of the AI to handle real-world applications.\\n\\n#### 3. Address Ethical Considerations\\n\\nGenerative AI can raise ethical issues, particularly around transparency, privacy, and the potential for misuse. For instance, deepfakes created by generative models can be used for misinformation. Therefore, it's essential to establish ethical guidelines on the use of generative AI technologies. Implementing robust data handling policies and maintaining transparency about AI-driven outputs are fundamental steps in this direction.\\n\\n#### 4. Testing and Validation\\n\\nBefore full-scale deployment, generative AI systems must undergo rigorous testing and validation. This involves not only technical assessments to ensure the system works as intended under various conditions but also evaluations to ensure that the outputs meet quality standards and do not perpetrate harmful biases or errors.\\n\\n#### 5. Scalability and Infrastructure\\n\\nDeploying generative AI applications at scale requires significant computational resources. Organizations need to plan for the necessary infrastructure to support these demands, which might include cloud services or specialized hardware. Moreover, the scalability plan should accommodate increased loads and the possibility of further expanding the AI’s capabilities without degradation in performance.\\n\\n#### 6. Continuous Monitoring and Iteration\\n\\nOnce a generative AI application is in production, continuous monitoring becomes crucial. This allows teams to detect and correct issues like drift in model performance or unexpected user interactions. Regularly updating the model based on new data and feedback also helps in maintaining the relevance and effectiveness of the application.\\n\\n#### 7. Legal Compliance and Security\\n\\nLegal and security concerns must be addressed to protect the integrity of the system and the privacy of the users. Adhering to relevant laws and regulations concerning data protection (such as GDPR) is mandatory. Additionally, deploying robust security measures to thwart potential cyber threats is critical.\\n\\n#### Conclusion\\n\\nTaking generative AI from a promising prototype to a full-fledged production system is an exciting yet complex process. It involves thorough planning, ethical consideration, strategic testing, and continuous improvement. By focusing on these key areas, organizations can harness the power of generative AI to innovate, enhance operational efficiency, and offer unprecedented value to customers. As we step further into this AI-driven era, the potential for generative AI continues to expand, promising new opportunities and challenges alike.\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    ")\n",
    "total_time=time.time()-start_time\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1BGrCiYlVGa",
    "outputId": "f54aa9f3-ddc9-4ef6-c5ac-c0ab5fdbf02b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.365209748406716"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.usage.total_tokens/total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9UsLsA0rl9lR"
   },
   "outputs": [],
   "source": [
    "total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLoCH8zLmdRH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
