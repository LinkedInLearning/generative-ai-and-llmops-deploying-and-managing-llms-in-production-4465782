{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Challenge: Evaluating LLM Systems\n",
        "\n",
        "In this challenge, you will use the RAGAs framework to evaluate a RAG application.\n",
        "\n",
        "As a bonus you can also try to change the prompt or the contextual data then generate new answers and measure the metrics again. This will help you guide your prompt engineering process.\n",
        "\n",
        "[![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LinkedInLearning/generative-ai-and-llmops-deploying-and-managing-llms-in-production-4465782/blob/main/ch-05/challenge_evaluating_LLM_systems.ipynb)"
      ],
      "metadata": {
        "id": "IMFnZzEyGqR8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-zNdaFhrCSx"
      },
      "outputs": [],
      "source": [
        "# Install the RAGAS package\n",
        "!pip install ragas -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from ragas.metric import faithfulness, answer_relevancy, context_recall, context_precision\n",
        "from raga import evaluate\n",
        "import os\n",
        "import getpass"
      ],
      "metadata": {
        "id": "0BC-A6hArE2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Set your OpenAI Key\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "77kqJFXureym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO Fetch RAG Question, Answer and Context dataset"
      ],
      "metadata": {
        "id": "pxmrUhbHrGse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO Evaluate the dataset using RAGAs"
      ],
      "metadata": {
        "id": "XhXU1uSrrLTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y6GTGe0JrO5p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}