{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution: Deploying a simple RAG Application using an API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LinkedInLearning/generative-ai-and-llmops-deploying-and-managing-llms-in-production-4465782/blob/solution/ch-03/challenge_deploy_RAG_using_API.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N24LWygmAWMR",
    "outputId": "c467abc7-3d57-4461-dc57-b65bcb1c69f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphlit-client\n",
      "  Downloading graphlit_client-1.0.20240515002-py3-none-any.whl (138 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpx (from graphlit-client)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (2.7.1)\n",
      "Requirement already satisfied: PyJWT in /usr/lib/python3/dist-packages (from graphlit-client) (2.3.0)\n",
      "Collecting websockets (from graphlit-client)\n",
      "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (4.11.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (3.7.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx->graphlit-client)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (3.7)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (1.3.1)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->graphlit-client)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->graphlit-client) (1.2.1)\n",
      "Installing collected packages: websockets, h11, httpcore, httpx, graphlit-client\n",
      "Successfully installed graphlit-client-1.0.20240515002 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 websockets-12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install graphlit-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpHXBdHLAdUp"
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from graphlit import Graphlit\n",
    "from graphlit_api import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7EwkV4FpAebz"
   },
   "outputs": [],
   "source": [
    "graphlit = Graphlit(organization_id=\"\", environment_id=\"\", jwt_secret=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAPi3B4cAflm"
   },
   "outputs": [],
   "source": [
    "async def create_feed(graphlit, uri):\n",
    "    input = FeedInput(\n",
    "        name=uri,\n",
    "        type=FeedTypes.WEB,\n",
    "        web=WebFeedPropertiesInput(\n",
    "            uri=uri,\n",
    "            readLimit=10\n",
    "        )\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = await graphlit.client.create_feed(input)\n",
    "\n",
    "        feed_id = response.create_feed.id\n",
    "    except GraphQLClientError as e:\n",
    "        return str(e)\n",
    "\n",
    "    return feed_id\n",
    "\n",
    "async def create_specification(graphlit):\n",
    "    input = SpecificationInput(\n",
    "        name=\"Summarization\",\n",
    "        type=SpecificationTypes.COMPLETION,\n",
    "        serviceType=ModelServiceTypes.ANTHROPIC,\n",
    "        searchType=SearchTypes.VECTOR,\n",
    "        anthropic=AnthropicModelPropertiesInput(\n",
    "            model=AnthropicModels.CLAUDE_3_HAIKU,\n",
    "            temperature=0.1,\n",
    "            probability=0.2,\n",
    "            completionTokenLimit=2048,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = await graphlit.client.create_specification(input)\n",
    "\n",
    "        spec_id = response.create_specification.id\n",
    "    except GraphQLClientError as e:\n",
    "        return str(e)\n",
    "\n",
    "    return spec_id\n",
    "\n",
    "async def create_conversation(graphlit, spec_id):\n",
    "    input = ConversationInput(\n",
    "        name=\"Conversation\",\n",
    "        specification=EntityReferenceInput(\n",
    "            id=spec_id\n",
    "        )\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = await graphlit.client.create_conversation(input)\n",
    "\n",
    "        conv_id = response.create_conversation.id\n",
    "    except GraphQLClientError as e:\n",
    "        return str(e)\n",
    "\n",
    "    return conv_id\n",
    "\n",
    "async def prompt_conversation(graphlit, conv_id, prompt):\n",
    "    try:\n",
    "        response = await graphlit.client.prompt_conversation(prompt, conv_id)\n",
    "\n",
    "        message = response.prompt_conversation.message.message\n",
    "        citations = response.prompt_conversation.message.citations\n",
    "\n",
    "        return message, citations\n",
    "    except GraphQLClientError as e:\n",
    "        return None, str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "kKc3vLKPAhEC",
    "outputId": "24543f00-f0f9-4bf2-f60f-e22cb2909fc5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'80700023-4aa2-4e44-9ab2-e5fc008c4958'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uri='https://lilianweng.github.io/posts/2023-06-23-agent/'\n",
    "feed_id=await create_feed(graphlit, uri)\n",
    "feed_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "yxe1zDYIAiT9",
    "outputId": "9e8309ec-34a3-4c4e-f9ac-c978b85ebaf0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'dabb0ba5-2063-46a2-b205-406e8666ab90'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_id= await create_specification(graphlit)\n",
    "spec_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "QIYaXmVdAkll",
    "outputId": "3c003671-91f0-4201-b9c2-b5f80308c191"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'9ae4fe1a-3ac0-40d2-a06b-b3b5086412cd'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_id = await create_conversation(graphlit, spec_id)\n",
    "conv_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uNL6KGBLAmfI",
    "outputId": "f4b37eaa-c8f0-43bb-c676-d8898e6a9138"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Chain of Thought (CoT) and Tree of Thoughts (ToT) are two different prompting techniques for enhancing the performance of large language models on complex tasks.\\n\\nChain of Thought, proposed by Wei et al. in 2022, instructs the model to 'think step-by-step' to decompose a hard task into smaller, more manageable steps. This allows the model to utilize more test-time computation and provide an interpretation of its thinking process.\\n\\nTree of Thoughts, proposed by Yao et al. in 2023, extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be breadth-first or depth-first, with each state evaluated by a classifier or majority vote.\\n\\nIn summary, CoT focuses on sequential step-by-step reasoning, while ToT explores a tree of possible reasoning paths, providing more flexibility and exploration of alternative solutions to complex problems.\",\n",
       " None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await prompt_conversation(graphlit, conv_id, \"What is the difference between chain of thought and tree of thought and who created them?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvG7Z1nGApoY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
